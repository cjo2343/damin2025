{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3846d67",
   "metadata": {},
   "source": [
    "# Data Integration from Multiple Sources\n",
    "\n",
    "This notebook demonstrates how to integrate data from different sources using schema alignment and merging techniques. We'll work with data from CSV and XML files, showing how to handle different formats and schemas.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand data integration challenges\n",
    "- Learn to parse different data formats (CSV, XML)\n",
    "- Master schema alignment techniques\n",
    "- Implement intelligent data merging strategies\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python programming\n",
    "- Familiarity with pandas and data structures\n",
    "- Understanding of XML structure\n",
    "- Knowledge of data merging concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4842d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c99641",
   "metadata": {},
   "source": [
    "## Step 1: Load CSV Data (Sales Database)\n",
    "\n",
    "We'll start by loading customer sales data from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c844e630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Customer Sales Data:\n",
      "============================================================================================\n",
      "   customer_id        name  sales_amount  sales_date\n",
      "0            1  Customer 1           202  2023-01-01\n",
      "1            2  Customer 2           535  2023-01-02\n",
      "2            3  Customer 3           960  2023-01-03\n",
      "3            4  Customer 4           370  2023-01-04\n",
      "4            5  Customer 5           206  2023-01-05\n",
      "============================================================================================\n",
      "    customer_id          name  sales_amount  sales_date\n",
      "95           96   Customer 96           301  2023-04-06\n",
      "96           97   Customer 97           369  2023-04-07\n",
      "97           98   Customer 98           962  2023-04-08\n",
      "98           99   Customer 99           915  2023-04-09\n",
      "99          100  Customer 100           370  2023-04-10\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reading the CSV file (Sales Database)\n",
    "df_sales = pd.read_csv(\"./data/customer_sales.csv\")\n",
    "\n",
    "print(\"Original Customer Sales Data:\")\n",
    "print(\"=\" * 92)\n",
    "print(df_sales.head())\n",
    "print(\"=\" * 92)\n",
    "print(df_sales.tail())\n",
    "print(\"=\" * 92)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6010826",
   "metadata": {},
   "source": [
    "## Step 2: Define XML Parser Function\n",
    "\n",
    "We need a custom function to parse XML data and convert it to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5dda30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Parse an XML file and return a DataFrame with the data.\n",
    "    \n",
    "    Args:\n",
    "        file (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the data from the XML file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract the data from the XML file\n",
    "    data = []\n",
    "    for customer in root.findall(\"customer\"):\n",
    "        id_ = int(customer.find(\"id\").text)\n",
    "        name = customer.find(\"full_name\").text\n",
    "        email = customer.find(\"email\").text\n",
    "        purchase = customer.find(\"purchase\").text\n",
    "        signup_date = customer.find(\"signup_date\").text\n",
    "        data.append([id_, name, email, purchase, signup_date])\n",
    "\n",
    "    # Creating a DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\n",
    "        \"customer_id\", \"name\", \"email\", \"sales_amount\", \"signup_date\"\n",
    "    ])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2e24f",
   "metadata": {},
   "source": [
    "## Step 3: Load XML Data (Marketing Database)\n",
    "\n",
    "Now we'll use our parser function to load customer marketing data from an XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff18af8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Customer Marketing Data:\n",
      "============================================================================================\n",
      "   customer_id         name                   email sales_amount signup_date\n",
      "0            2   Customer 2   customer2@example.com          535  2023-01-02\n",
      "1            6   Customer 6   customer6@example.com          171  2023-01-06\n",
      "2           10  Customer 10  customer10@example.com          221  2023-01-10\n",
      "3           16  Customer 16  customer16@example.com          472  2023-01-16\n",
      "4           17  Customer 17  customer17@example.com          199  2023-01-17\n",
      "============================================================================================\n",
      "    customer_id          name                    email sales_amount  \\\n",
      "95          171  Customer 171  customer171@example.com          851   \n",
      "96          172  Customer 172  customer172@example.com          243   \n",
      "97          173  Customer 173  customer173@example.com          708   \n",
      "98          174  Customer 174  customer174@example.com          300   \n",
      "99          175  Customer 175  customer175@example.com          223   \n",
      "\n",
      "   signup_date  \n",
      "95  2023-04-12  \n",
      "96  2023-04-13  \n",
      "97  2023-04-14  \n",
      "98  2023-04-15  \n",
      "99  2023-04-16  \n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reading the XML file (Marketing Database)\n",
    "df_marketing = parse_xml(\"./data/customer_marketing.xml\")\n",
    "\n",
    "print(\"\\nOriginal Customer Marketing Data:\")\n",
    "print(\"=\" * 92)\n",
    "print(df_marketing.head())\n",
    "print(\"=\" * 92)\n",
    "print(df_marketing.tail())\n",
    "print(\"=\" * 92)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d33886",
   "metadata": {},
   "source": [
    "## Step 4: Schema Alignment\n",
    "\n",
    "Before merging, we need to align the schemas by converting columns to appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0cc44c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after schema alignment:\n",
      "\n",
      "Sales DataFrame dtypes:\n",
      "customer_id              int64\n",
      "name                    object\n",
      "sales_amount             int64\n",
      "sales_date      datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Marketing DataFrame dtypes:\n",
      "customer_id              int64\n",
      "name                    object\n",
      "email                   object\n",
      "sales_amount             int64\n",
      "signup_date     datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Schema Alignment: Convert columns to appropriate types\n",
    "df_marketing[\"sales_amount\"] = pd.to_numeric(\n",
    "    df_marketing[\"sales_amount\"], errors=\"coerce\").fillna(0)\n",
    "df_sales[\"sales_date\"] = pd.to_datetime(df_sales[\"sales_date\"])\n",
    "df_marketing[\"signup_date\"] = pd.to_datetime(df_marketing[\"signup_date\"])\n",
    "\n",
    "print(\"Data types after schema alignment:\")\n",
    "print(\"\\nSales DataFrame dtypes:\")\n",
    "print(df_sales.dtypes)\n",
    "print(\"\\nMarketing DataFrame dtypes:\")\n",
    "print(df_marketing.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ad9ed",
   "metadata": {},
   "source": [
    "## Step 5: Intelligent Data Merging\n",
    "\n",
    "We'll merge the data from both sources, handling duplicate columns intelligently by prioritizing certain data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01d3a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial merged DataFrame shape: (175, 8)\n",
      "Columns after initial merge: ['customer_id', 'name_sales', 'sales_amount_sales', 'sales_date', 'name_marketing', 'email', 'sales_amount_marketing', 'signup_date']\n"
     ]
    }
   ],
   "source": [
    "# Merging the CSV and XML data intelligently\n",
    "# Merge on \"customer_id\", prioritize \"df_sales\" data when duplicate columns exist\n",
    "df_merged = pd.merge(df_sales, df_marketing, on=\"customer_id\", how=\"outer\",\n",
    "                     suffixes=(\"_sales\", \"_marketing\"))\n",
    "\n",
    "print(\"Initial merged DataFrame shape:\", df_merged.shape)\n",
    "print(\"Columns after initial merge:\", df_merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822360e3",
   "metadata": {},
   "source": [
    "## Step 6: Resolve Duplicate Columns\n",
    "\n",
    "We'll resolve duplicate columns by prioritizing non-null values from the sales data and filling missing values with marketing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49b60ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved duplicate columns successfully\n"
     ]
    }
   ],
   "source": [
    "# Resolve the duplicated columns by:\n",
    "#  - Prioritizing non-null values from the sales data\n",
    "#  - Filling missing values with data from the marketing dataset\n",
    "df_merged[\"name\"] = df_merged[\"name_sales\"].combine_first(\n",
    "    df_merged[\"name_marketing\"]\n",
    ")\n",
    "df_merged[\"sales_amount\"] = df_merged[\"sales_amount_sales\"].combine_first(\n",
    "    df_merged[\"sales_amount_marketing\"]\n",
    ")\n",
    "df_merged[\"sales_date\"] = df_merged[\"sales_date\"].combine_first(\n",
    "    df_merged[\"signup_date\"]\n",
    ")\n",
    "\n",
    "print(\"Resolved duplicate columns successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62037ce3",
   "metadata": {},
   "source": [
    "## Step 7: Clean Up and Reorder Columns\n",
    "\n",
    "We'll remove the temporary columns with suffixes and reorder the final DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8900836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame shape: (175, 5)\n",
      "Final columns: ['customer_id', 'name', 'email', 'sales_amount', 'sales_date']\n"
     ]
    }
   ],
   "source": [
    "# Dropping the unnecessary columns (those with suffixes)\n",
    "df_merged = df_merged.drop(columns=[\n",
    "    \"name_sales\",\n",
    "    \"name_marketing\",\n",
    "    \"sales_amount_sales\",\n",
    "    \"sales_amount_marketing\",\n",
    "    \"signup_date\"\n",
    "])\n",
    "\n",
    "# Reorder the columns\n",
    "df_merged = df_merged.reindex(columns=[\n",
    "    \"customer_id\",\n",
    "    \"name\",\n",
    "    \"email\",\n",
    "    \"sales_amount\",\n",
    "    \"sales_date\"\n",
    "])\n",
    "\n",
    "print(\"Final DataFrame shape:\", df_merged.shape)\n",
    "print(\"Final columns:\", df_merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4914d66",
   "metadata": {},
   "source": [
    "## Step 8: Display the Unified Schema\n",
    "\n",
    "Let's examine the final integrated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8e1d3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unified Data after Schema Integration:\n",
      "============================================================================================\n",
      "   customer_id        name                  email  sales_amount sales_date\n",
      "0            1  Customer 1                    NaN         202.0 2023-01-01\n",
      "1            2  Customer 2  customer2@example.com         535.0 2023-01-02\n",
      "2            3  Customer 3                    NaN         960.0 2023-01-03\n",
      "3            4  Customer 4                    NaN         370.0 2023-01-04\n",
      "4            5  Customer 5                    NaN         206.0 2023-01-05\n",
      "============================================================================================\n",
      "     customer_id          name                    email  sales_amount  \\\n",
      "170          171  Customer 171  customer171@example.com         851.0   \n",
      "171          172  Customer 172  customer172@example.com         243.0   \n",
      "172          173  Customer 173  customer173@example.com         708.0   \n",
      "173          174  Customer 174  customer174@example.com         300.0   \n",
      "174          175  Customer 175  customer175@example.com         223.0   \n",
      "\n",
      "    sales_date  \n",
      "170 2023-04-12  \n",
      "171 2023-04-13  \n",
      "172 2023-04-14  \n",
      "173 2023-04-15  \n",
      "174 2023-04-16  \n",
      "============================================================================================\n",
      "\n",
      "Data Integration Summary:\n",
      "- Original Sales records: 100\n",
      "- Original Marketing records: 100\n",
      "- Integrated records: 175\n",
      "- Data types: {'customer_id': dtype('int64'), 'name': dtype('O'), 'email': dtype('O'), 'sales_amount': dtype('float64'), 'sales_date': dtype('<M8[ns]')}\n"
     ]
    }
   ],
   "source": [
    "# Display the unified schema\n",
    "print(\"\\nUnified Data after Schema Integration:\")\n",
    "print(\"=\" * 92)\n",
    "print(df_merged.head())\n",
    "print(\"=\" * 92)\n",
    "print(df_merged.tail())\n",
    "print(\"=\" * 92)\n",
    "\n",
    "print(f\"\\nData Integration Summary:\")\n",
    "print(f\"- Original Sales records: {len(df_sales)}\")\n",
    "print(f\"- Original Marketing records: {len(df_marketing)}\")\n",
    "print(f\"- Integrated records: {len(df_merged)}\")\n",
    "print(f\"- Data types: {df_merged.dtypes.to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47025262",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we successfully demonstrated data integration from multiple sources:\n",
    "\n",
    "### Key Steps Accomplished:\n",
    "\n",
    "1. **Data Loading**: Loaded data from CSV and XML files using appropriate parsers\n",
    "2. **Schema Alignment**: Converted data types to ensure compatibility\n",
    "3. **Intelligent Merging**: Combined datasets using outer join to preserve all records\n",
    "4. **Conflict Resolution**: Prioritized data from sales database while filling gaps with marketing data\n",
    "5. **Schema Unification**: Created a clean, unified schema with consistent column names\n",
    "\n",
    "### Challenges Addressed:\n",
    "\n",
    "- **Format Diversity**: Handled different file formats (CSV vs XML)\n",
    "- **Schema Differences**: Aligned column names and data types\n",
    "- **Data Conflicts**: Resolved duplicate information intelligently\n",
    "- **Missing Values**: Filled gaps using data from multiple sources\n",
    "\n",
    "### Best Practices Demonstrated:\n",
    "\n",
    "- **Prioritization Strategy**: Sales data took precedence over marketing data\n",
    "- **Data Validation**: Used error handling for numeric conversions\n",
    "- **Clean Organization**: Removed temporary columns and reordered for clarity\n",
    "\n",
    "This approach ensures data quality while maximizing information retention from all available sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "damin2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
